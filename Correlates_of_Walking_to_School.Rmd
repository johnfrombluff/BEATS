---
title: "Correlates of Walking to School"
author: "John Williams"
output: 
  html_document:
    toc: true
    number_sections: true
    theme: yeti
    css: style.css
  Grmd::docx_document:
    fig_caption: TRUE
    force_captions: TRUE
    toc: TRUE
    toc_depth: 5
    css: style.css 
  pdf_document:
    toc: true
    latex_engine: xelatex
    keep_tex: true
bibliography: /home/john/Dropbox/Writing/bib/all-refs.bib
csl: /home/john/Dropbox/Writing/bib/CSL/apa.csl

---

```{r label=R-setup, echo=FALSE, include=FALSE, cache=FALSE}

# bibliography: /home/john/Dropbox/Writing/bib/all-refs.bib
require( ggplot2 )
require( scales )
require( Gmisc )
require( rms )
require( knitr )
require( JMisc )
# Evaluate the figure caption after the plot
knitr::opts_knit$set(eval.after='fig.cap')

# Use the table counter that the htmlTable() provides
options(table_counter = TRUE)

# Use the figCapNo() with roman letters
options(fig_caption_no_roman = TRUE)

#theme_set( theme_gray( base_size = 10 ))
#theme_update( legend.key.width = unit( 3,"line") )
options(width=120)
options("show.signif.stars"=F)

## set global chunk options
opts_chunk$set(echo=FALSE,
               cache=TRUE,
               dpi=96,
               fig.height=5,
               fig.width=7,
               prompt=F,
               tidy=T,
               highlight=T,
               dev="png",
               dev.args=list(type="cairo"),
               fig.align='center',
               fig.show='hold',
               par=TRUE,
               comment=NA,
               background="wheat",
               prompt=FALSE,
               warning=FALSE,
               message=FALSE)

info   <- sessionInfo()
r_ver  <- paste(info$R.version$major, info$R.version$minor, sep=".")
barcol <- "wheat"
require(JMisc)
```

# Notes to the research team
**Update**: I'm pressed for time and updating most of the material below is low
priority. So please ignore it and skip to the section "Modelling" for the 
current work.


This file contains two things:

1. A record of the analyses that I've done, with full details of the statistical
results.  More details can be shown on request
2. Snippets that can be copied and pasted into manuscripts.  (Note the abstract 
is at the end because the numbers in the abstract rely on code occurs earlier in
this file.)

All analyses were performed using **R**, ver. `r r_ver` [@R15] with packages 
rms, ver. `r info$otherPkgs$rms$Version` [@rms15] for  analysis; 
Gmisc, ver. `r info$otherPkgs$Gmisc$Version` for plot and table output; and 
knitr, ver `r info$otherPkgs$knitr$Version` [@xie14] for reproducible research.

## Reproducible research
This file is automatically generated, i.e. all the in-text numbers, tables and
graphs are generated from the data.  If the data file changes, or we decide to 
analyse a different subset of the data, or include or exclude certain variables,
the content in  the document will change automatically (almost), i.e. without
the need to re-type anything.

"Weaving" text, code and results together and rendering them into a document is 
known as "reproducible research", i.e. there is code in the source file that 
reads the data and performs the analysis, then generates the document. This 
makes your research report more tightly bound to the data, and makes it less 
likely that you will be unable to reproduce or extend your research in the 
future if the need arises (as it often does, in my experience). You can open 
the file with the same name as this one, but the extension ".Rmd" 
to see the  source file which is compiled into HTML by **R**. 

This approach can also generate PDF (which some journals accept), but as yet 
there is no one-step  method for rendering directly to an editable format, e.g. 
odt, rtf or docx. But  it's easy to simply copy and paste from your web-browser 
into your word-processor. But of course you should not do this until everyone
involved  with the manuscript preparation is happy that the results are 100% 
finalised!

## Data Setup
The **R** code below is just to show which data file is being used.

```{r label="read-data", echo=2:4, size="small", prompt=FALSE, cache=TRUE}
require(foreign)
dir  <- "/home/john/Dropbox/Research/Collaboration/BEATS/John/W2S"
#file <- "BEATS_SS_ForWalk2School_150507.sav"
file <- "BEATS_SS_ForWalk2School_160201_COMPLETEdata.sav"
dat <- read.spss( paste(dir, file, sep="/"), to.data.frame=TRUE )
rm(dir, file )
```


```{r label=data-setup, results='hide', cache=TRUE}
source( "data-setup-W2S.R")
```

# Introduction

We know from previous work (and common sense!) that the most influential 
correlate is distance from school. But once that is factored out, what else is 
influential?

```{r label=ATS-Dist, dependson="data-setup", fig.cap="Figure 1: Empirical probability of ATS by distance from school", fig.align='center'}
emp <- NULL
plot_cutoff <- 10000

with( dat.full[ dat.full$Dist2School <= plot_cutoff, ],
      {
        for ( i in 1:length(Dist2School ) ) 
           emp[i] <- prop.table(table(ATS_f[ Dist2School < Dist2School[i]]))["Walk"]
        emp_prob <- data.frame( dist=Dist2School, true=ATS_f, prob=emp )
        p <- ggplot( emp_prob, aes( dist, prob) ) 
        p <- p + labs(x="Distance to school (m)", y="Probability of Walking to School" )
        p <- p + geom_point(size=1) 
        p <- p + geom_smooth( size=1, method="gam", formula= y ~ s( x, bs = "cs") )
        p <- p + annotate("text", label="Optimal distance\nthreshold: ≤ 2,200m\nSensitivity: 84%\nSpecificity: 84%\nAUC: 93%", x=4000, y=0.8, hjust="l")
        p <- p + geom_vline(xintercept=2200) 
        print(p)
        }
      )
```

The figure above shows the empirical probability of walking to school, i.e. the 
proportion of respondents walk at each level of distance to school. The 
observations plotted are those respondents who live less than 
`r I(format(plot_cutoff, big.mark=","))`m from school to make the plot more interpretable. The blue 
line is a Generalised Additive Model smoother.  

```{r walkstats, results='asis'}
walk_min <- 469 
walk_50  <- 3100
walk_max <- 5800 

tmp <- dat$ATS_f[dat$Dist2School <= walk_min]
walk_min_n <- sum(table(tmp))

tmp <- dat$ATS_f[dat$Dist2School <= walk_50  ]
walk_50_n  <- sum(table(tmp))

tmp <- dat$ATS_f[dat$Dist2School > walk_max  ]
walk_max_n <- sum(table(tmp))
```

Of the  `r I(walk_min_n)` students who live less than `r I(walk_min)`m from 
school, all  walk to school. Of the `r I(walk_50_n)` who live less than 
`r I(walk_50)`m, 50% walk.  Finally, of the `r I(walk_max_n)` students who live
more than `r I(walk_max)`m from school, none of them walk to school.

# Sample description

## Exclusion of cases and missing value analysis

The analyses below are restricted to the students who are not boarders. There are
`r I(format(dim(dat.ats)[ 1 ], big.mark=","))` students who fit those criteria. 
The table below shows the number of missing values on variables to be included 
in the multivariate  analyses, which reduce the available sample size.

<center>
```{r mva, dependson="data-setup"}
# 2 obs  (9078, 9038) had missing aGender but non-missing gender; and missing 
# age_cat but non-missing school year, DOB_day, DOB year ats. I entered 12.5 
# for Age_Cat
# 4 (9165, 4002, 1034, 10152) had missing values for all WSC sections etc. 
# Maybe they didn't finish the questionnaire?
# 11 (9165, 4002, 1034, 10152, 9104, 3027, 9010, 8141, 8175, 3087, 5108) have
# missing value on W2S section
mva( dat.ats, thr= 4 )
```
</center>

The variables with 11 cases missing are due to participants not completing a 
section of the questionnaire (due to time constraints?).  Not all students
consented to, or had time available for, anthropometry, so many of these values 
are missing. It's not clear to me why there are so many missing values for the 
other variables though.

A total of `r I(fmt(dim(dat.ats)[ 1 ]))` adolescents were 
included in the analysis (Age
`r I(round(mean(dat.ats$Age_at_Survey, na.rm=T),1))` ± 
`r I(round(sd(dat.ats$Age_at_Survey, na.rm=T),1))` years; 
`r I(round(100*prop.table(table(dat.ats$gender)),1)["Male"])`% boys; 
`r I(round(100*prop.table(table(dat.ats$eth3)),1)["NZ European"])`% New Zealand European; 
`r I(round(100*prop.table(table(dat.ats$BMI_f)),1)["Normal"])`% normal weight). 
The most common modes of transport to school was being driven by others 
(`r I(sum(round(100*prop.table(table(dat.ats$TscCarOth)),1)[4:5]))`%) 
followed by walking 
(`r I(sum(round(100*prop.table(table(dat.ats$TscWalk)),1)[4:5]) )`%), 
school bus (`r I(sum(round(100*prop.table(table(dat.ats$TscBusSc)),1)[4:5]))`%), 
public bus (`r I(sum(round(100*prop.table(table(dat.ats$TscBusPub)),1)[4:5]))`%)
and driving themselves 
(`r I(sum(round(100*prop.table(table(dat.ats$TscCarMy)),1)[4:5]))`%).
Overall, 
`r I(round(100*prop.table(table(dat.ats$ATS)),1)[1])`%
of adolescents used motorized transport only, 
`r I(round(100*prop.table(table(dat.ats$ATS)),1)[2])`% 
W2S, and 
`r I(round(100*prop.table(table(dat.ats$ATS)),1)[3])`% 
used a combination of motorized and active transport to school. Most students 
(`r I(round(100*prop.table(table(dat.ats$TSlike)),1)["Yes"])`%) 
liked the way they travelled to school. 


## Potential correlates 
Socio-demographic characteristics of students who walked versus did not walk to 
school are presented in Tables 2, 3 and 4.

<center>
```{r label="descriptives-demos", dependson="data-setup" }
require( Gmisc )
n.w  <- table(dat.ats$W2S)[2]
n.dw <- table(dat.ats$W2S)[1]
res  <- data_setup( dat.ats,  "W2S", c("Age_at_Survey", "Sex", "eth3", 
                                         "BMI_4cat", "cars3", "NZDepCat3", 
                                         "tsdecision", "Dist"))
(tab1 <- htmlTable(x = res$tab,
  rgroup   = res$rgroup,
  n.rgroup = res$ngroup,
  label    = "Table1",
  caption  = "Demographic characteristics of the sample.",
  tfoot    = "<small><sup>&dagger;</sup>Categorical variables are reported in counts and percentages: count (%). The <i>p</i>-values are from Fisher tests. Proportions are calculated horizontally. Continuous variables are reported as mean (±SD). The <i>p</i>-values are from Wilcoxon tests.</small>",
  rowlabel = "Variable<sup>&dagger;</sup>",
  css.rgroup = "font-weight: 100",
  #cgroup   = c(n.dw,             n.w, ""),
  #n.cgroup = c(1,                1, 1 ),
  ctable   = TRUE ))
```
</center>

The tables below are not intended for inclusion in manuscripts, they are just 
here for reference in case we decide to change anything else.

<center>
```{r label="descriptives-cat"}
require( Gmisc ) 
res <- data_setup( dat.ats, "W2S", c("School", "BMI_4cat", "HMcars", "ScrGuide",
                                       "whodecides", "schiclose"))
(tab1 <- htmlTable(x = res$tab,
  rgroup   = res$rgroup,
  n.rgroup = res$ngroup,
  label    = "Table1",
  caption  = "Categorical individual and household potential correlates of walking to school",
  tfoot    = "<small><sup>&dagger;</sup>Variables are reported in counts and percentages: count (%). The <i>p</i>-values are from Fisher tests. The total proportions are calculated vertically, and the others are calulated horizontally.</small>",
  rowlabel = "Variable<sup>&dagger;</sup>",
  css.rgroup = "font-weight: 100",
  ctable   = TRUE ))
rm( tab1 )
```
</center>

&nbsp;

<center>
```{r descriptives-cont}
require( Gmisc )
res <- data_setup( dat.ats , "W2S", names(dat.ats)[c(16, 13:14, 17:50)])
(tab2 <- htmlTable(x = res$tab,
  rgroup   = res$rgroup,
  n.rgroup = res$ngroup,
  label    = "Table1",
  caption  = "Continuous individual potential correlates of walking to school",
  tfoot    = "<small><sup>&dagger;</sup>The <i>p</i>-values are from  Wilcoxon tests.</small>",
  rowlabel = "Variable<sup>&dagger;</sup>",
  css.rgroup = "font-weight: 100",
  ctable   = TRUE ))
rm( tab2 )
```
</center>

# Modeling
Sandy asked me to do this in SPSS, but that software appears to be unable to
deal with cluster sampling in the same way that Stata and **R** can, namely by
adjusting the standard errors of the generalized linear mixed model.  Also 
adopting an alternative approach using the GLMM module in SPSS and structuring
the data as a hierarchical mixed linear model with schools as the top level 
resulted in a model that could not be estimated due to numerical problems, 
*even* for the most simple models, i.e. all the "moderators" (distance, age and
sex) plus one other potential covariate. This is not solely an SPSS problem, as
the same problems were encountered using SAS.  Perhaps SPSS can do what we need,
but a day of research and trial-and-error could not discover that.
 
So, given the tight deadline (4 days) and the fact that I already wasted one day
finding out out that SPSS couldn't do the job, I elected to use **R**.  The
process was this:

1. All observations in the file with a value of the variable **Include** equal 
   to 1 were included.  No restrictions based on distance were used.
1. Every potential covariate on the list that Sandy supplied was ran as a 
   "univariate" model in the sense above (i.e. the variable in question, plus
   the "moderators")
1. All the covariates that were significant at the 5% level were entered in a 
   multivariate model.
1. This model could not be estimated.  The offending variables were the Tg*, 
   variables, i.e. "Travel in general".  These were removed and the model was
   estimated to convergence.
1. A stepwise process was followed (manually) whereby at each stage the variable
   with the largest *p*-value was removed, the model re-estimated and the 
   estimates compared.
1. As soon as possible during this process, the Tg* variables were re-introduced.
1. This procedure resulted in the models below, which show the variables that
   remained significant at the 5% level, and the 1% level. Given the relatively
   large sample size, large number of IVs and large number of hypothesis tests,
   I strongly recommend using the 1% level.

Notes:

- The effect of distance on the probability of walking to school is clearly 
non-linear, so the distance variable was transformed using a restricted cubic 
spline (with three knots).
- Because the data were collected within schools, robust standard errors were 
calculated using school as a cluster variable. 
- Following @hosmer13 [p. 177], models with areas under the  ROC curve greater 
than 0.9 are labelled "outstanding", and those with ROC areas between 0.8 and 
0.9 are labelled "excellent".

## Model 0 
Here are the results of including all the significant "univariate" correlates,
except the Tg* variables.

```{r model.0, dependson="data-setup"}
dat.m <- dat[ complete.cases(dat$Include), ]
m.ddist <- datadist(dat.m)
options(datadist='m.ddist')
m0 <- robcov( lrm(ATS_f ~ Dist2School + gender + Age_at_Survey + as.numeric(HMcars) + as.numeric(WSpsh) + as.numeric(WSpunsafe) + as.numeric(WSpwalk) + as.numeric(WSAint) + as.numeric(WSAnice) + as.numeric(WSAstim) + as.numeric(WSAnice) + as.numeric(WSAgood) + as.numeric(WSAuseful) + as.numeric(WSAsafe) + as.numeric(WCSone) + as.numeric(WSchat) + as.numeric(WSfsh) + as.numeric(WSbfri) + as.numeric(WSf5ws) + as.numeric(WSbwant) + as.numeric(WStired) + as.numeric(WSno) + as.numeric(WSbstuff) + as.numeric(WSbsweat) + as.numeric(WSbplan) + as.numeric(WSbsched) + as.numeric(Dlikedriven) + as.numeric(F3_landuseaccess) + as.numeric(WSbdist) + as.numeric(WStime) + as.numeric(WSbsafe) + as.numeric(WSbfootp) + as.numeric(WCShills) + as.numeric(WCSrbor) + as.numeric(TSlike) + as.numeric(WScontrol) + as.numeric(WSintend) + as.numeric(WSwant) + as.numeric(WSconf) + as.numeric(WS2wks) + as.numeric(Icardrive) + as.numeric(Icarown) + as.numeric(hsiblings) + as.numeric(raincoat) + as.numeric(Datuse), x=T, y=T, data=dat.m ), cluster=dat.m$school)
```

```{r m0-show, dependson="data-setup"}
#aov2html(m0)
m0
```

## Model 1 (Significant at 5%)
Following best practice (not *common* practice), as explained by 
@harrell01 [pp. 56--60], all significant univariate correlates from  were 
included in the initial model. This model was reduced by removing the correlate 
with the largest *p*-value one at a time and re-inspecting the fit carefully. 
This resulted in the model shown below.

<center>
```{r ATS-m1, dependson=c("read-data"), results='asis'}
m1.dat <- dat.m[ complete.cases(dat.m$ATS_f, dat.m$Dist2School, dat.m$gender, dat.m$Age_at_Survey, dat.m$WSAstim, dat.m$WSchat, dat.m$WSno, dat.m$WCSrbor, dat.m$WScontrol, dat.m$WSintend, dat.m$WS2wks, dat.m$Datuse),]
m1.ddist <- datadist( m1.dat )
options( datadist = "m1.ddist", contrasts=c("contr.treatment", "contr.treatment") )

m1 <- robcov(lrm(ATS_f ~ rcs(Dist2School, 3) + gender + Age_at_Survey + as.numeric(WSAstim) + as.numeric(WSchat) + as.numeric(WSno) + as.numeric(WCSrbor) + as.numeric(WScontrol) + as.numeric(WSintend) + as.numeric(WS2wks) + as.numeric(Datuse), x=T, y=T, data=m1.dat ), cluster=m1.dat$school)
m1.gof <- LR.summary(m1, print=FALSE)
htmlTable( m1.gof$sf, 
          rnames=F, 
          align="r", 
          header=c("Index", "<span style='padding:5mm'/>", "Statistic", "<span style='padding:5mm'/>"), 
          caption="Model 1 goodness of fit and summary information")
aov2html( m1 )
```
</center>
  
## Model 2 (Significant at the 1% level)
The 5% level leaves to much room  for Type I error due to sample size and alpha 
inflation, so the 1% level was used to avoid these problems.  Regardless of 
*common practice* this is a wise  strategy to avoid false positives and 
non-replicable scientific research [@ioannidis05].

<center>
```{r ATS-m2, dependson=c("data-setup", "ATS-m1"), results='asis'}
dat.m$WSno     <- as.numeric(dat.m$WSno)
dat.m$WCSrbor  <- as.numeric(dat.m$WCSrbor)
dat.m$WSintend <- as.numeric(dat.m$WSintend)
dat.m$WS2wks   <- as.numeric(dat.m$WS2wks)

m2.dat <- dat.m[ complete.cases(dat.m$ATS_f, dat.m$school,  dat.m$Dist2School, dat.m$gender, dat.m$Age_at_Survey, dat.m$WSno, dat.m$WCSrbor, dat.m$WSintend, dat.m$WS2wks), c("ATS_f", "school", "Dist2School", "gender", "Age_at_Survey", "WSno", "WCSrbor", "WSintend", "WS2wks")]
m2.ddist <- datadist( m2.dat )
options( datadist = "m2.ddist", contrasts=c("contr.treatment", "contr.treatment") )

m2 <- robcov(lrm(ATS_f ~ rcs(Dist2School, 3) + gender + Age_at_Survey + WSno + WCSrbor + WSintend + WS2wks, x=T, y=T, data=m2.dat ), cluster=m2.dat$school)

m2.gof <- LR.summary( m2, print=FALSE )
htmlTable( m2.gof$sf, 
          rnames=F, 
          align="r", 
          header=c("Index", "<span style='padding:5mm'/>", "Statistic", "<span style='padding:5mm'/>"), 
          caption="Model 2 goodness of fit and summary information")
aov2html( m2 )
```
</center>

Table `r I(options("table_counter"))`  shows Wald tests of the covariates. 


```{r ATS-final, dependson=c("ATS-m2", "data-setup"), results='asis'}
m_final <- m2
m_final_gof <- LR.summary( m_final, print=FALSE )
#htmlTable(round(anova(m_final), 3))
```

To address the possibility of over-fitting, bias-corrected goodness of fit 
indices were calculated using `r validate_n<-20; I(validate_n)` bootstrap 
samples. (**NB**: currently this is set rather low, so that this file will
compile  quickly. I will update it to be larger if required, but I've previously 
inspected the calibration results with *n*=200, giving the same substantive 
conclusion.)

<center>
```{r ATS-final-validate, dependson="ATS-final", results='asis'} 
validate_n <- 100
s <- validate( m_final, B =validate_n )
colnames(s) <- c("Model", "Training", "Test", "Optimism", "Corrected", "n")
rownames(s)[1] <- "R<sub>xy</sub>"
rownames(s)[2] <- "R<sup>2</sup>"
htmlTable( round(s, 3), 
           caption="Model bootstrap calibration", 
           rowlabel="Index",
           align="r",
           label="m_final_calibration")
```
</center>

Table `r I(options("table_counter"))` shows that the bias-corrected indices  are 
very similar to the model indices, hence the results are unlikely to be due to 
over-fitting, and are more likely to be generalisable to new samples from the 
same population.

<center>
```{r m2-results-table, results='asis', dependson="ATS-final"}
#LR.summary(m_final, table=T, print=F)
options( datadist = "m2.ddist", contrasts=c("contr.treatment", "contr.treatment") )
htmlTable( OR( m2, Dist=c(4000,5000)), 
          label="OR-M2", 
          caption="Odds ratios<sup>&dagger;</sup>",
          align="r",
          tfoot="<small><sup>&dagger;</sup> Odds ratios are functions of the difference column, not the usual 1-unit calculation. The low and high values are the IQRs (except for distance, where the low and high values were chosen for clarity of interpretation).</small>")
```
</center>

Table `r I(options("table_counter"))` shows the odds ratios from Model 2 and 
Figure 2 below represents the odds ratios and their confidence intervals 
graphically.

```{r ATS-OR-plot, dependson="ATS-final", fig.width=7.5, fig.cap="Figure 2: Odds ratios"}
plot(summary(m_final, Dist=c(4000,5000)) )
```

Figure 3 below shows how the log odds of W2S varies over the range of the 
correlates in the final model, with shaded areas indicating 95% confidence 
intervals.

```{r ATS-OR-logit-plot, dependson="ATS-final",fig.width=8.5, fig.cap="Figure 3: Effect of covariates on logit"}
res <- Predict( m_final )
#plot(res[res$.predictor != "Dist2School", ])
plot( res )
m4.gof <- LR.summary(update(m_final, . ~ . - rcs(Dist, 3)), print=F )
```

## Summary
The final model has absolutely outstanding predictive and discriminant validity, 
and all  effects can be interpreted *ceteris paribus*. It also has good face 
validity. 

But it is somewhat trivial and non-informative from the point of view of 
designing interventions to encourage walking to school. It basically tells us 
what every social psychologist and consumer behaviourist knows: the best 
predictor of behaviour is past behaviour and future intentions.  In this 
particular case, the other explanatory variables are also somewhat trivial,
being reflective of a stereotypical teenager who, when asked why they do not do
something, will reply either "I can't be bothered" or "It's boring". So, sadly, 
all this tells us very little about how behaviour could be modified for 
beneficial outcomes.


# References
